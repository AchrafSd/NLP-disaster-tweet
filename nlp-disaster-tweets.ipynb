{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-10T14:28:40.273440Z","iopub.execute_input":"2022-01-10T14:28:40.273974Z","iopub.status.idle":"2022-01-10T14:28:44.662635Z","shell.execute_reply.started":"2022-01-10T14:28:40.273860Z","shell.execute_reply":"2022-01-10T14:28:44.660981Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:44.664270Z","iopub.execute_input":"2022-01-10T14:28:44.665018Z","iopub.status.idle":"2022-01-10T14:28:44.748543Z","shell.execute_reply.started":"2022-01-10T14:28:44.664980Z","shell.execute_reply":"2022-01-10T14:28:44.747809Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.keyword.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:44.749760Z","iopub.execute_input":"2022-01-10T14:28:44.750019Z","iopub.status.idle":"2022-01-10T14:28:44.765361Z","shell.execute_reply.started":"2022-01-10T14:28:44.749982Z","shell.execute_reply":"2022-01-10T14:28:44.764255Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.location.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:44.767560Z","iopub.execute_input":"2022-01-10T14:28:44.767923Z","iopub.status.idle":"2022-01-10T14:28:44.779884Z","shell.execute_reply.started":"2022-01-10T14:28:44.767867Z","shell.execute_reply":"2022-01-10T14:28:44.778936Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### Lowercasing","metadata":{}},{"cell_type":"code","source":"train[\"keyword\"] = train[\"keyword\"].apply(lambda x : str.lower(x) if pd.isna(x) != True else x)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : str.lower(x) if pd.isna(x) != True else x)\ntrain[\"location\"] = train[\"location\"].apply(lambda x : str.lower(x) if pd.isna(x) != True else x)\n\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x : str.lower(x) if pd.isna(x) != True else x)\ntest[\"text\"] = test[\"text\"].apply(lambda x : str.lower(x) if pd.isna(x) != True else x)\ntest[\"location\"] = test[\"location\"].apply(lambda x : str.lower(x) if pd.isna(x) != True else x)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:44.781354Z","iopub.execute_input":"2022-01-10T14:28:44.781602Z","iopub.status.idle":"2022-01-10T14:28:44.826649Z","shell.execute_reply.started":"2022-01-10T14:28:44.781567Z","shell.execute_reply":"2022-01-10T14:28:44.825856Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:44.828482Z","iopub.execute_input":"2022-01-10T14:28:44.828761Z","iopub.status.idle":"2022-01-10T14:28:44.841634Z","shell.execute_reply.started":"2022-01-10T14:28:44.828717Z","shell.execute_reply":"2022-01-10T14:28:44.840483Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Entities, URL Links and Punctuation Removal","metadata":{}},{"cell_type":"code","source":"import re,string\n\ndef remove_entities(tweet):\n    entity_prefixes = [\"@\", \"#\"]\n    for separator in string.punctuation:\n        if separator not in entity_prefixes:\n            tweet = tweet.replace(separator,\" \") #replaceing every punctuation symbol with a space, except for the hashtags and tags.\n    words = []\n    for word in tweet.split(\" \"):\n        word = word.strip() #removing uncessary sapces at the start and end of each word\n        if word:\n            if word[0] not in entity_prefixes:\n                words.append(word) #adding the word the list words only if its not a hashtag or a tag\n    return \" \".join(words)\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x : remove_entities(x) if pd.isna(x) != True else x)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : remove_entities(x) if pd.isna(x) != True else x)\ntrain[\"location\"] = train[\"location\"].apply(lambda x : remove_entities(x) if pd.isna(x) != True else x)\n\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x : remove_entities(x) if pd.isna(x) != True else x)\ntest[\"text\"] = test[\"text\"].apply(lambda x : remove_entities(x) if pd.isna(x) != True else x)\ntest[\"location\"] = test[\"location\"].apply(lambda x : remove_entities(x) if pd.isna(x) != True else x)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:44.842681Z","iopub.execute_input":"2022-01-10T14:28:44.843008Z","iopub.status.idle":"2022-01-10T14:28:45.178324Z","shell.execute_reply.started":"2022-01-10T14:28:44.842973Z","shell.execute_reply":"2022-01-10T14:28:45.177632Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Remove url links\nimport re\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x : re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", ' ', x) if pd.isna(x) != True else x)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", ' ', x) if pd.isna(x) != True else x)\ntrain[\"location\"] = train[\"location\"].apply(lambda x : re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", ' ', x) if pd.isna(x) != True else x)\n\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x : re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", ' ', x) if pd.isna(x) != True else x)\ntest[\"text\"] = test[\"text\"].apply(lambda x : re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", ' ', x) if pd.isna(x) != True else x)\ntest[\"location\"] = test[\"location\"].apply(lambda x : re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", ' ', x) if pd.isna(x) != True else x)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:45.179580Z","iopub.execute_input":"2022-01-10T14:28:45.179812Z","iopub.status.idle":"2022-01-10T14:28:45.283482Z","shell.execute_reply.started":"2022-01-10T14:28:45.179780Z","shell.execute_reply":"2022-01-10T14:28:45.282768Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#remove punctuation\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x : re.sub(r\"[^\\w\\s]\", \" \", x) if pd.isna(x) != True else x)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : re.sub(r\"[^\\w\\s]\", \" \", x) if pd.isna(x) != True else x)\ntrain[\"location\"] = train[\"location\"].apply(lambda x : re.sub(r\"[^\\w\\s]\", \" \", x) if pd.isna(x) != True else x)\n\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x : re.sub(r\"[^\\w\\s]\", \" \", x) if pd.isna(x) != True else x)\ntest[\"text\"] = test[\"text\"].apply(lambda x : re.sub(r\"[^\\w\\s]\", \" \", x) if pd.isna(x) != True else x)\ntest[\"location\"] = test[\"location\"].apply(lambda x : re.sub(r\"[^\\w\\s]\", \" \", x) if pd.isna(x) != True else x)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:45.284790Z","iopub.execute_input":"2022-01-10T14:28:45.285047Z","iopub.status.idle":"2022-01-10T14:28:45.385411Z","shell.execute_reply.started":"2022-01-10T14:28:45.285014Z","shell.execute_reply":"2022-01-10T14:28:45.384695Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"pip install symspellpy","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:45.388256Z","iopub.execute_input":"2022-01-10T14:28:45.388515Z","iopub.status.idle":"2022-01-10T14:28:54.994290Z","shell.execute_reply.started":"2022-01-10T14:28:45.388481Z","shell.execute_reply":"2022-01-10T14:28:54.993490Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#spelling correction\n\nfrom symspellpy import SymSpell, Verbosity\n\nsym_spell = SymSpell()\n\ndictionary_path = \"./frequency_dictionary_en_82_765.txt\"\n\nsym_spell.load_dictionary(dictionary_path, 0, 1)\n\ndef spelling_correction(sent):\n    doc_w_correct_spelling = []\n    for tok in sent.split(\" \"):\n        \n        x = sym_spell.lookup(tok, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)[0].__str__()\n        y = x.split(',')[0]\n        doc_w_correct_spelling.append(y)\n        \n    return \" \".join(doc_w_correct_spelling)\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x : spelling_correction(x) if pd.isna(x) != True else x)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : spelling_correction(x) if pd.isna(x) != True else x)\ntrain[\"location\"] = train[\"location\"].apply(lambda x : spelling_correction(x) if pd.isna(x) != True else x)\n\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x : spelling_correction(x) if pd.isna(x) != True else x)\ntest[\"text\"] = test[\"text\"].apply(lambda x : spelling_correction(x) if pd.isna(x) != True else x)\ntest[\"location\"] = test[\"location\"].apply(lambda x : spelling_correction(x) if pd.isna(x) != True else x)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:54.995832Z","iopub.execute_input":"2022-01-10T14:28:54.996158Z","iopub.status.idle":"2022-01-10T14:28:56.009143Z","shell.execute_reply.started":"2022-01-10T14:28:54.996122Z","shell.execute_reply":"2022-01-10T14:28:56.008401Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Filling missing data","metadata":{}},{"cell_type":"code","source":"train.info()\ntest.info()\n#missing data in both sets","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:56.010340Z","iopub.execute_input":"2022-01-10T14:28:56.010592Z","iopub.status.idle":"2022-01-10T14:28:56.041710Z","shell.execute_reply.started":"2022-01-10T14:28:56.010558Z","shell.execute_reply":"2022-01-10T14:28:56.040870Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:28:56.043075Z","iopub.execute_input":"2022-01-10T14:28:56.043347Z","iopub.status.idle":"2022-01-10T14:29:05.938865Z","shell.execute_reply.started":"2022-01-10T14:28:56.043309Z","shell.execute_reply":"2022-01-10T14:29:05.937994Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Keyword extraction to fill the missing data in the keyword column\n\nimport spacy #advanced NLP library\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport os\nos.system('python -m spacy download en')\nnlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:29:05.940765Z","iopub.execute_input":"2022-01-10T14:29:05.941071Z","iopub.status.idle":"2022-01-10T14:29:29.075462Z","shell.execute_reply.started":"2022-01-10T14:29:05.941025Z","shell.execute_reply":"2022-01-10T14:29:29.074721Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\n\nmodel = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nembed = hub.load(model)\n\ndef extract_keywords(nlp=nlp, doc=\"\", no_of_keywords=5, model=model):\n    doc = doc.lower()\n    doc = re.sub(r'(?:\\@|http?\\://|https?\\://|www)\\S+', ' ', doc)\n    doc = re.sub(r'[^\\w\\s]', ' ', doc)\n    doc = re.sub(' \\d+ ', ' ', doc)\n    \n    doc_ = nlp(doc)\n    \n    #list of word categories (parts-of-speech tags)\n    pos_tag = ['VERB', 'NOUN', 'ADJ', 'PROPN']\n    result = []\n    \n    for token in doc_:\n        if(toekn.pos_ in pos_tag):\n            result.append(token.text)\n            \n    doc_embedding = model.encode([doc])\n    results_embeddings = model.encode(result)\n    \n    #calculate similarity between document and results embeddings\n    distances = cosine_similarity(doc_embedding, results_embeddings)\n    \n    #get the top similar keywords\n    keywords = [result[index] for index in distances.argsort()[0][-no_of_keywords:]]\n    \n    return keywords","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:29:29.076728Z","iopub.execute_input":"2022-01-10T14:29:29.076994Z","iopub.status.idle":"2022-01-10T14:29:40.410991Z","shell.execute_reply.started":"2022-01-10T14:29:29.076959Z","shell.execute_reply":"2022-01-10T14:29:40.410242Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#fill the empty entries in train[keyword]\nfor i in range(len(train[\"keyword\"])):\n    if pd.isnull(train[\"keyword\"].iloc[i]):\n        try:\n            train[\"keyword\"].iloc[i] = extract_keywords(nlp=nlp, doc=train.text.iloc[i], no_of_keywords=1, model=model)[0]\n        except:\n            train[\"keyword\"].iloc[i] = \"NaN\"\n\n#fill the empty entries in test[keyword]\nfor i in range(len(test[\"keyword\"])):\n    if pd.isnull(test[\"keyword\"].iloc[i]):\n        try:\n            test[\"keyword\"].iloc[i] = extract_keywords(nlp=nlp, doc=test.text.iloc[i], no_of_keywords=1, model=model)[0]\n        except:\n            test[\"keyword\"].iloc[i] =\"NaN\"","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:29:40.412323Z","iopub.execute_input":"2022-01-10T14:29:40.412577Z","iopub.status.idle":"2022-01-10T14:29:41.110145Z","shell.execute_reply.started":"2022-01-10T14:29:40.412543Z","shell.execute_reply":"2022-01-10T14:29:41.109421Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Entity recognition\n\ndef get_location(nlp=nlp, doc=\"\"):\n    doc_ = nlp(doc)\n    \n    location = \"\"\n    \n    for ent in doc_.ents:\n        if ent.label in [\"GPE\", \"ORG\"]: #if the entity's label is a geopolitical entity\n            location = location + ent.text + \" \"\n    return location\n#fill the empty entities in train[\"location\"]\nfor i in range(len(train[\"location\"])):\n    if pd.isnull(train[\"location\"].iloc[i]):\n        try:\n            train[\"location\"].iloc[i] = get_location(nlp=nlp, doc=train.text.iloc[i])\n        except:\n            test[\"location\"].iloc[i] = \"NaN\"\n\n#fill the empty entities in test[\"location\"]\nfor i in range(len(test[\"location\"])):\n    if pd.isnull(test[\"location\"].iloc[i]):\n        try:\n            test[\"location\"].iloc[i] = get_location(nlp=nlp, doc=test.iloc[i])\n        except:\n            test[\"location\"].iloc[i] = \"NaN\"","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:29:41.111336Z","iopub.execute_input":"2022-01-10T14:29:41.111574Z","iopub.status.idle":"2022-01-10T14:29:59.391599Z","shell.execute_reply.started":"2022-01-10T14:29:41.111540Z","shell.execute_reply":"2022-01-10T14:29:59.390829Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Lemmatization\n\nThe goal of lemmatization is to convert a word to its root form","metadata":{}},{"cell_type":"code","source":"def lemmatize(sentence):\n    doc = nlp(sentence)\n    lemmas = [token.lemma_ for token in doc]\n    return \" \".join(lemmas)\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x : lemmatize(x) if pd.isna(x) != True else x)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : lemmatize(x) if pd.isna(x) != True else x)\ntrain[\"location\"] = train[\"location\"].apply(lambda x : lemmatize(x) if pd.isna(x) != True else x)\n\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x : lemmatize(x) if pd.isna(x) != True else x)\ntest[\"text\"] = test[\"text\"].apply(lambda x : lemmatize(x) if pd.isna(x) != True else x)\ntest[\"location\"] = test[\"location\"].apply(lambda x : lemmatize(x) if pd.isna(x) != True else x)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:29:59.393101Z","iopub.execute_input":"2022-01-10T14:29:59.393345Z","iopub.status.idle":"2022-01-10T14:32:24.428613Z","shell.execute_reply.started":"2022-01-10T14:29:59.393311Z","shell.execute_reply":"2022-01-10T14:32:24.427647Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Stop words removal\n\ndef remove_stopwords(sentence):\n    doc = nlp(sentence)\n    all_stopwords = nlp.Defaults.stop_words\n    doc_tokens = [token.text for token in doc]\n    tokens_without_sw = [word for word in doc_tokens if not word in all_stopwords]\n    return \" \".join(tokens_without_sw)\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x : remove_stopwords(x) if pd.isna(x) != True else x)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : remove_stopwords(x) if pd.isna(x) != True else x)\ntrain[\"location\"] = train[\"location\"].apply(lambda x : remove_stopwords(x) if pd.isna(x) != True else x)\n\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x : remove_stopwords(x) if pd.isna(x) != True else x)\ntest[\"text\"] = test[\"text\"].apply(lambda x : remove_stopwords(x) if pd.isna(x) != True else x)\ntest[\"location\"] = test[\"location\"].apply(lambda x : remove_stopwords(x) if pd.isna(x) != True else x)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:32:24.430306Z","iopub.execute_input":"2022-01-10T14:32:24.430585Z","iopub.status.idle":"2022-01-10T14:34:47.237876Z","shell.execute_reply.started":"2022-01-10T14:32:24.430547Z","shell.execute_reply":"2022-01-10T14:34:47.237056Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### Model","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\ntrain = shuffle(train, random_state=42).reset_index(drop=True) #shuffling training data\ny = np.array(train[\"target\"].tolist())\n\nkey_embed = embed(train.keyword.to_list())\nloc_embed = embed(train.location.to_list())\ntext_embed = embed(train.text.to_list())","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:34:47.239320Z","iopub.execute_input":"2022-01-10T14:34:47.239586Z","iopub.status.idle":"2022-01-10T14:34:47.338370Z","shell.execute_reply.started":"2022-01-10T14:34:47.239550Z","shell.execute_reply":"2022-01-10T14:34:47.337594Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Flatten, concatenate, Dropout\nfrom tensorflow.keras import Model\n\nkeyword_input = Input(shape=(key_embed.shape[1],))\nlocation_input = Input(shape=(loc_embed.shape[1],))\ntext_input = Input(shape=(text_embed.shape[1],))\n\n#create keyword model\nkey_model = Flatten()(keyword_input)\nkey_model = Dense(1024, activation='relu')(key_model)\nkey_model = Dropout(0.5)(key_model)\n\n#create location model\nloc_model = Flatten()(location_input)\nloc_model = Dense(1024, activation='relu')(loc_model)\nloc_model = Dropout(0.5)(loc_model)\n\n#create text model\ntext_model = Flatten()(text_input)\ntext_model = Dense(1024, activation='relu')(text_model)\ntext_model = Dropout(0.5)(text_model)\n\n#concatenate the three models\nmerged = concatenate([key_model,\n                         loc_model,\n                             text_model], axis=1)\nmerged = Dense(1024, activation='relu')(merged)\nmerged = Dropout(0.5)(merged)\nfinal = Dense(1, activation='sigmoid')(merged)\n\nfinal = Model(inputs = [keyword_input, location_input, text_input], outputs=final)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:35:37.620569Z","iopub.execute_input":"2022-01-10T14:35:37.621318Z","iopub.status.idle":"2022-01-10T14:35:37.691713Z","shell.execute_reply.started":"2022-01-10T14:35:37.621281Z","shell.execute_reply":"2022-01-10T14:35:37.690876Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"lr = 0.1 #learning rate\nepochs = 100 #number of epochs\nopt = tf.keras.optimizers.SGD(lr = lr, momentum = 0.8, decay = lr/epochs) #optimizer\n\nfinal.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nearlystop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', mode = 'min', patience=10, verbose = 1\n)\n\nhistory = final.fit(\n            x = [key_embed, loc_embed, text_embed],\n            y = y,\n            batch_size = 32,\n            epochs = epochs,\n#             callbacks = [earlystop],\n            validation_split = 0.1\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:38:07.079442Z","iopub.execute_input":"2022-01-10T14:38:07.080186Z","iopub.status.idle":"2022-01-10T14:39:40.938667Z","shell.execute_reply.started":"2022-01-10T14:38:07.080144Z","shell.execute_reply":"2022-01-10T14:39:40.937938Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:43:30.716532Z","iopub.execute_input":"2022-01-10T14:43:30.716794Z","iopub.status.idle":"2022-01-10T14:43:30.731020Z","shell.execute_reply.started":"2022-01-10T14:43:30.716763Z","shell.execute_reply":"2022-01-10T14:43:30.730317Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test = shuffle(test, random_state=42).reset_index(drop=True) #shuffling training data\n\nkey_embed_test = embed(test.keyword.to_list())\nloc_embed_test = embed(test.location.to_list())\ntext_embed_test = embed(test.text.to_list())","metadata":{"execution":{"iopub.status.busy":"2022-01-10T14:56:00.652518Z","iopub.execute_input":"2022-01-10T14:56:00.652823Z","iopub.status.idle":"2022-01-10T14:56:00.692925Z","shell.execute_reply.started":"2022-01-10T14:56:00.652789Z","shell.execute_reply":"2022-01-10T14:56:00.692101Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"prediction = np.round(final.predict([key_embed_test, loc_embed_test, text_embed_test]))\nsample_submission[\"target\"] = prediction.astype('int64')","metadata":{"execution":{"iopub.status.busy":"2022-01-10T15:03:20.967446Z","iopub.execute_input":"2022-01-10T15:03:20.967950Z","iopub.status.idle":"2022-01-10T15:03:21.108183Z","shell.execute_reply.started":"2022-01-10T15:03:20.967893Z","shell.execute_reply":"2022-01-10T15:03:21.107439Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"sample_submission.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-10T15:07:13.733853Z","iopub.execute_input":"2022-01-10T15:07:13.734404Z","iopub.status.idle":"2022-01-10T15:07:13.741430Z","shell.execute_reply.started":"2022-01-10T15:07:13.734366Z","shell.execute_reply":"2022-01-10T15:07:13.740767Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T15:03:25.610677Z","iopub.execute_input":"2022-01-10T15:03:25.611483Z","iopub.status.idle":"2022-01-10T15:03:25.625245Z","shell.execute_reply.started":"2022-01-10T15:03:25.611441Z","shell.execute_reply":"2022-01-10T15:03:25.624525Z"},"trusted":true},"execution_count":77,"outputs":[]}]}